# MQUANT: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization

![MQUANT Logo](https://github.com/yourusername/mquant/blob/main/assets/logo.png)

## Overview

**MQUANT** is a state-of-the-art framework designed to optimize the inference efficiency of multimodal large language models (LLMs) through comprehensive static quantization techniques. By significantly reducing model size and computational overhead, MQUANT enables faster and more cost-effective deployments of sophisticated AI models without compromising their performance.

This repository accompanies our ICLR 2024 paper titled **"MQUANT: UNLEASHING THE INFERENCE POTENTIAL OF MULTIMODAL LARGE LANGUAGE MODELS VIA FULL STATIC QUANTIZATION"**. The proposed quantization methods are meticulously evaluated on the Qwen2-VL-7B model, showcasing substantial improvements in both efficiency and accuracy.

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Contributing](#contributing)
- [Citation](#citation)
- [License](#license)
- [Contact](#contact)
- [Acknowledgements](#acknowledgements)

## Features

- **Full Static Quantization**: Comprehensive quantization of both language and visual components in multimodal LLMs.
- **AIFS and MDQ Integration**: Advanced techniques to enhance the performance of LLMs under quantization.
- **LN2RN Transformation**: Innovative transformations for improved visual component quantization.
- **RMS Scaling**: Fine-tuning strategies to closely match the performance of full-precision BF16 models.
- **Performance Optimization**: Achieves near full-precision performance while significantly reducing model size and inference latency.

## Installation

*Coming Soon.*

Detailed instructions for setting up the MQUANT framework will be provided once the code is released. Stay tuned for updates!

## Usage

*Coming Soon.*

Comprehensive usage examples and tutorials will be available with the code release to help you get started with MQUANT effortlessly.

## Contributing

We welcome contributions from the research and development community! Whether you're interested in improving the existing features, adding new functionalities, or reporting issues, your input is invaluable.

Please refer to the [CONTRIBUTING.md](https://github.com/yourusername/mquant/blob/main/CONTRIBUTING.md) file for guidelines on how to contribute to this project.

## Citation

If you find MQUANT useful in your research, please consider citing our paper:

```bibtex
@inproceedings{mquant2024,
  title={MQUANT: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization},
  author={Your Name and Co-authors},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://openreview.net/forum?id=yourpaperid}
}
